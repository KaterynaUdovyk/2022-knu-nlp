{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing all necessary libraries and our data"
      ],
      "metadata": {
        "id": "uT_dQOE10cQi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHc3F-2ugRAm",
        "outputId": "ce19777b-ca68-42fc-9042-313029ccff99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OdFPqLZgzxT",
        "outputId": "0de000a9-7048-4df1-aeb3-3c2a9c460e7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WUFR76Xsgy22"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GxhHlAcwgzs_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/TheSocialDilemma/TheSocialDilemma.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spliting our data into 2 parts   80/20"
      ],
      "metadata": {
        "id": "zNuw4wTK0kxd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ni83q9kfg44B"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['Sentiment'], test_size=0.20)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing neutral comments"
      ],
      "metadata": {
        "id": "oLwO0_cY0qf3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1cZqqRCGg7p5"
      },
      "outputs": [],
      "source": [
        "X_train = X_train[(df.Sentiment == 'Positive') | (df.Sentiment == 'Negative')]\n",
        "X_test = X_test[(df.Sentiment == 'Positive') | (df.Sentiment == 'Negative')]\n",
        "y_train = y_train[(df.Sentiment == 'Positive') | (df.Sentiment == 'Negative')]\n",
        "y_test = y_test[(df.Sentiment == 'Positive') | (df.Sentiment == 'Negative')]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaninig text"
      ],
      "metadata": {
        "id": "NT0cwTLl0tBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8G9PP3_rg-jR"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NCCmlqFuhBII"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"@\\S+\", \" \", text)\n",
        "    text = re.sub(\"https*\\S+\", \" \", text)\n",
        "    text = re.sub(\"#\\S+\", \" \", text)\n",
        "    text = re.sub(\"\\d\", \" \", text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\s{2,}',' ', text)\n",
        "    text = ' '.join([word for word in text.split(' ') if word not in stop_words])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0ulnR_a_hDaR"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.apply(lambda x:clean_text(x))\n",
        "X_test = X_test.apply(lambda x:clean_text(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting \"Positive\" and \"Negative\" into 1.0 and 0.0 for further calculations"
      ],
      "metadata": {
        "id": "jQIcQhCP0wUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tpt1dyGDhJCr"
      },
      "outputs": [],
      "source": [
        "def MakingLabel(text):\n",
        "  if text == \"Positive\":\n",
        "    return 1.0\n",
        "  if text == \"Negative\":\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Bkx60LvZhK3e"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.apply(lambda x:MakingLabel(x))\n",
        "y_test = y_test.apply(lambda x:MakingLabel(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a dictionary of the whole text to create sequences"
      ],
      "metadata": {
        "id": "lLq_q4aB00d6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uJfbocJRhNZJ"
      },
      "outputs": [],
      "source": [
        "text = ''\n",
        "for i in X_train:\n",
        "    text += i + ' '\n",
        "for i in X_test:\n",
        "    text += i + ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "pCI3YMNVhQ0E"
      },
      "outputs": [],
      "source": [
        "def tokenization(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JchfNFMjhSs0"
      },
      "outputs": [],
      "source": [
        "def lemmatizer(text):\n",
        "    lemm_text = [WordNetLemmatizer().lemmatize(word) for word in text]\n",
        "    return lemm_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5e3tJnjZhVZC"
      },
      "outputs": [],
      "source": [
        "dict_full = list(set(lemmatizer(tokenization(text))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making sequences"
      ],
      "metadata": {
        "id": "uI-XB2Ue03Wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fVb5gL6EitQ2"
      },
      "outputs": [],
      "source": [
        "def sequences(text):\n",
        "    sequence = [dict_full.index(i) for i in lemmatizer(tokenization(text))]\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "uQVwTWUmhW5w"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.apply(lambda x:sequences(x))\n",
        "X_test = X_test.apply(lambda x:sequences(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing sequences"
      ],
      "metadata": {
        "id": "cwP068wn06Xh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ItX5OzIshYKp"
      },
      "outputs": [],
      "source": [
        "def vectorize_sequences(sequences, dimension=11000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        for j in sequence:\n",
        "            results[i, j] = 1.\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "VOjBkvo9haE5"
      },
      "outputs": [],
      "source": [
        "X_train = vectorize_sequences(X_train)\n",
        "X_test = vectorize_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_Mg1zEyPhbZo"
      },
      "outputs": [],
      "source": [
        "y_train = np.asarray(y_train).astype(\"float32\")\n",
        "y_test = np.asarray(y_test).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making our model"
      ],
      "metadata": {
        "id": "V7EYhUdD08m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 120\n",
        "\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "0isNZBNKvMFt"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zr9KykcRhcyf"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "X1YMlcVai9Sq"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "56zjJUOhjAMT"
      },
      "outputs": [],
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "vocab_size = 11000\n",
        "\n",
        "\n",
        "inputs = layers.Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(30, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "6xRbpHUKonBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d04db1e-cde0-4532-ed97-3c87787753b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "328/328 [==============================] - 36s 102ms/step - loss: 0.6038 - accuracy: 0.7204 - val_loss: 0.5833 - val_accuracy: 0.7301\n",
            "Epoch 2/5\n",
            "328/328 [==============================] - 31s 93ms/step - loss: 0.5954 - accuracy: 0.7261 - val_loss: 0.5832 - val_accuracy: 0.7301\n",
            "Epoch 3/5\n",
            "328/328 [==============================] - 33s 100ms/step - loss: 0.5921 - accuracy: 0.7263 - val_loss: 0.5836 - val_accuracy: 0.7301\n",
            "Epoch 4/5\n",
            "328/328 [==============================] - 32s 99ms/step - loss: 0.5908 - accuracy: 0.7262 - val_loss: 0.5835 - val_accuracy: 0.7301\n",
            "Epoch 5/5\n",
            "328/328 [==============================] - 31s 94ms/step - loss: 0.5912 - accuracy: 0.7263 - val_loss: 0.5839 - val_accuracy: 0.7301\n"
          ]
        }
      ],
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
        "\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model.fit(\n",
        "    X_train, y_train, batch_size=32, epochs=5, callbacks=[callback], validation_data=(X_test, y_test)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTYEV7pQyGtE",
        "outputId": "991b052b-3242-4d4e-9cb4-a676d974d460"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score: 0.5838561058044434\n",
            "Test accuracy: 0.7300613522529602\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Text classification with Transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}