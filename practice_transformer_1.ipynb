{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1297c0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd43b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOG file id =  1HqD1LueAC1We0Bgum4qNxxCU6VJZW3yW\n",
      "LOG download url =  https://drive.google.com/uc?id=1HqD1LueAC1We0Bgum4qNxxCU6VJZW3yW\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "#url_testDATA='https://drive.google.com/file/d/1q8_G19zYuD4C2l8u4CrGe_yiitjI1h2I/view?usp=sharing'\n",
    "url_trainDATA='https://drive.google.com/file/d/1HqD1LueAC1We0Bgum4qNxxCU6VJZW3yW/view?usp=sharing'\n",
    "\n",
    "#https://www.kaggle.com/datatattle/email-classification-nlp?select=SMS_train.csv\n",
    "url=url_trainDATA\n",
    "file_id=url.split('/')[-2]\n",
    "print('LOG file id = ',file_id)\n",
    "dwn_url='https://drive.google.com/uc?id=' + file_id\n",
    "print('LOG download url = ', dwn_url)\n",
    "\n",
    "df = pd.read_csv(dwn_url, encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "211184e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Message_body     Label\n",
      "0                           Rofl. Its true to its name  Non-Spam\n",
      "1    The guy did some bitching but I acted like i'd...  Non-Spam\n",
      "2    Pity, * was in mood for that. So...any other s...  Non-Spam\n",
      "3                 Will ü b going to esplanade fr home?  Non-Spam\n",
      "4    This is the 2nd time we have tried 2 contact u...      Spam\n",
      "..                                                 ...       ...\n",
      "952  hows my favourite person today? r u workin har...  Non-Spam\n",
      "953                      How much you got for cleaning  Non-Spam\n",
      "954  Sorry da. I gone mad so many pending works wha...  Non-Spam\n",
      "955                                 Wat time ü finish?  Non-Spam\n",
      "956                    Just glad to be talking to you.  Non-Spam\n",
      "\n",
      "[957 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.drop(['S. No.'], axis = 1, inplace = True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5413dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 140\n",
    "max_features = 30000\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "def digital_label(text):\n",
    "    if text == 'Non-Spam':\n",
    "        return 1.0\n",
    "    elif text == 'Spam':\n",
    "        return 0.0\n",
    "    \n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"@\\S+\", \" \", text)  \n",
    "    text = re.sub(\"https*\\S+\", \" \", text)\n",
    "    text = re.sub(\"www\\S+\", \" \", text)\n",
    "    text = re.sub(\"#\\S+\", \" \", text)\n",
    "    text = re.sub(\"\\d\", \" \", text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
    "    text = re.sub('\\n', ' ', text)        \n",
    "    text = re.sub('\\s{2,}',' ', text)\n",
    "    text = ' '.join([word for word in text.split(' ') if word not in stop_words])\n",
    "    return text\n",
    "\n",
    "def convert(text):\n",
    "    tok = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem = [lemmatizer.lemmatize(t) for t in tok]\n",
    "    res = [dictionary.index(i) for i in lem]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e05e1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Message_body     Label\n",
      "0                                       rofl true name  Non-Spam\n",
      "1    guy bitching acted like interested buying some...  Non-Spam\n",
      "2                               pity mood suggestions   Non-Spam\n",
      "3                         ü b going esplanade fr home   Non-Spam\n",
      "4    nd time tried contact u u £ pound prize claim ...      Spam\n",
      "..                                                 ...       ...\n",
      "952  hows favourite person today r u workin hard sl...  Non-Spam\n",
      "953                                  much got cleaning  Non-Spam\n",
      "954              sorry da gone mad many pending works   Non-Spam\n",
      "955                                 wat time ü finish   Non-Spam\n",
      "956                                      glad talking   Non-Spam\n",
      "\n",
      "[957 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.Message_body = df.Message_body.apply(clean_text)\n",
    "print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a52df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Message_body  Label\n",
      "0                                       rofl true name    1.0\n",
      "1    guy bitching acted like interested buying some...    1.0\n",
      "2                               pity mood suggestions     1.0\n",
      "3                         ü b going esplanade fr home     1.0\n",
      "4    nd time tried contact u u £ pound prize claim ...    0.0\n",
      "..                                                 ...    ...\n",
      "952  hows favourite person today r u workin hard sl...    1.0\n",
      "953                                  much got cleaning    1.0\n",
      "954              sorry da gone mad many pending works     1.0\n",
      "955                                 wat time ü finish     1.0\n",
      "956                                      glad talking     1.0\n",
      "\n",
      "[957 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "df.Label = df.Label.apply(digital_label)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77973f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "message:\n",
      " 0                                         rofl true name\n",
      "1      guy bitching acted like interested buying some...\n",
      "2                                 pity mood suggestions \n",
      "3                           ü b going esplanade fr home \n",
      "4      nd time tried contact u u £ pound prize claim ...\n",
      "                             ...                        \n",
      "952    hows favourite person today r u workin hard sl...\n",
      "953                                    much got cleaning\n",
      "954                sorry da gone mad many pending works \n",
      "955                                   wat time ü finish \n",
      "956                                        glad talking \n",
      "Name: Message_body, Length: 957, dtype: object\n",
      "\n",
      "label:\n",
      " 0      1.0\n",
      "1      1.0\n",
      "2      1.0\n",
      "3      1.0\n",
      "4      0.0\n",
      "      ... \n",
      "952    1.0\n",
      "953    1.0\n",
      "954    1.0\n",
      "955    1.0\n",
      "956    1.0\n",
      "Name: Label, Length: 957, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "X_train = df.Message_body\n",
    "Y_train = df.Label\n",
    "print(\"\\nmessage:\\n\", X_train)\n",
    "print(\"\\nlabel:\\n\", Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa57c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_text = \"\"\n",
    "for i in X_train:\n",
    "    whole_text += i + ' '\n",
    "tokens = word_tokenize(whole_text)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemtok = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "dictionary = list(set(lemtok))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eceba539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "convert\n",
      " 0                                       [985, 1267, 413]\n",
      "1      [1037, 999, 1360, 2035, 469, 1471, 864, 428, 1...\n",
      "2                                       [2263, 841, 890]\n",
      "3                       [984, 2000, 742, 997, 862, 1287]\n",
      "4      [2272, 82, 1109, 2317, 1209, 1209, 2350, 1765,...\n",
      "                             ...                        \n",
      "952    [1889, 1141, 91, 2314, 2543, 1209, 1395, 788, ...\n",
      "953                                    [265, 1504, 2455]\n",
      "954              [2361, 2347, 657, 503, 2307, 2395, 962]\n",
      "955                                  [419, 82, 984, 313]\n",
      "956                                         [1118, 1132]\n",
      "Name: Message_body, Length: 957, dtype: object\n",
      "\n",
      "sequences\n",
      " [[   0    0    0 ...  985 1267  413]\n",
      " [   0    0    0 ...  362 1209  518]\n",
      " [   0    0    0 ... 2263  841  890]\n",
      " ...\n",
      " [   0    0    0 ... 2307 2395  962]\n",
      " [   0    0    0 ...   82  984  313]\n",
      " [   0    0    0 ...    0 1118 1132]]\n",
      "\n",
      "massiv\n",
      " [1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 1. 1. 0.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 0. 1. 0.\n",
      " 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0.\n",
      " 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 1. 0. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.apply(convert)\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen = max_seq_len)\n",
    "y_train = np.asarray(Y_train).astype(\"float32\")\n",
    "print(\"\\nconvert\\n\", X_train)\n",
    "print(\"\\nsequences\\n\", x_train)\n",
    "print(\"\\nmassiv\\n\", y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e67bccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47c7b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(TokenAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e77a3e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 20  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 14  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(max_seq_len,))\n",
    "embedding_layer = TokenAndPositionEmbedding(max_seq_len, max_features, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b8b0ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "96/96 [==============================] - 7s 49ms/step - loss: 0.4105 - accuracy: 0.8600 - val_loss: 0.3574 - val_accuracy: 0.8725\n",
      "Epoch 2/6\n",
      "96/96 [==============================] - 4s 47ms/step - loss: 0.3681 - accuracy: 0.8704 - val_loss: 0.3303 - val_accuracy: 0.8725\n",
      "Epoch 3/6\n",
      "96/96 [==============================] - 4s 44ms/step - loss: 0.3369 - accuracy: 0.8662 - val_loss: 0.2917 - val_accuracy: 0.8725\n",
      "Epoch 4/6\n",
      "96/96 [==============================] - 4s 42ms/step - loss: 0.1483 - accuracy: 0.9457 - val_loss: 0.0360 - val_accuracy: 0.9864\n",
      "Epoch 5/6\n",
      "96/96 [==============================] - 4s 43ms/step - loss: 0.0479 - accuracy: 0.9864 - val_loss: 0.0162 - val_accuracy: 0.9948\n",
      "Epoch 6/6\n",
      "96/96 [==============================] - 4s 43ms/step - loss: 0.0232 - accuracy: 0.9937 - val_loss: 0.0062 - val_accuracy: 0.9979\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=10, epochs=6, validation_data=(x_train, y_train)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d0fae04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.006241454277187586\n",
      "Test accuracy: 0.9979101419448853\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_train, y_train, verbose=0) \n",
    "print('Test score:', score[0]) \n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e0fcb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
