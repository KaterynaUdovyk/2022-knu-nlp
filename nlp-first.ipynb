{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nthe task:\n\nuse the nltk library to preprocess the data:\n    remove stop words, punctuation\n    perform tokenization, stemming, or lemmatization\nuse the wordcloud to visualize:\n    the most common words\n    bi-gram\n        in each class\n        \nP.S. markdown sucks so imma use comments\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nimport pandas as pd\nimport re\nimport string\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:04.207771Z","iopub.execute_input":"2022-01-25T08:01:04.208054Z","iopub.status.idle":"2022-01-25T08:01:04.835982Z","shell.execute_reply.started":"2022-01-25T08:01:04.208025Z","shell.execute_reply":"2022-01-25T08:01:04.835187Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"path_to_tweets = \"../input/covid-19-nlp-text-classification/Corona_NLP_train.csv\"\ntweets = pd.read_csv(path_to_tweets, encoding = 'latin1')\ntweets.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:08.510389Z","iopub.execute_input":"2022-01-25T08:01:08.511016Z","iopub.status.idle":"2022-01-25T08:01:08.707723Z","shell.execute_reply.started":"2022-01-25T08:01:08.510975Z","shell.execute_reply":"2022-01-25T08:01:08.706939Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#all possible values for sentiment\ntweets[\"Sentiment\"].unique()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:12.780940Z","iopub.execute_input":"2022-01-25T08:01:12.781230Z","iopub.status.idle":"2022-01-25T08:01:12.791439Z","shell.execute_reply.started":"2022-01-25T08:01:12.781198Z","shell.execute_reply":"2022-01-25T08:01:12.790644Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\"\"\"\npositive and extremely positive I'll consider simply positive\nsame for (extremely) negative\nand I'll just drop out neutral ones to avoid confusing\n\"\"\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive = tweets[(tweets.Sentiment == 'Extremely Positive') | (tweets.Sentiment == 'Positive')]\nnegative = tweets[(tweets.Sentiment == 'Extremely Negative') | (tweets.Sentiment == 'Negative')]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:16.289994Z","iopub.execute_input":"2022-01-25T08:01:16.290600Z","iopub.status.idle":"2022-01-25T08:01:16.327443Z","shell.execute_reply.started":"2022-01-25T08:01:16.290564Z","shell.execute_reply":"2022-01-25T08:01:16.326581Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#for this task we don't need anything but tweets themselves\n#so for convinience\npositive = positive['OriginalTweet']\nnegative = negative['OriginalTweet']","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:20.517506Z","iopub.execute_input":"2022-01-25T08:01:20.517889Z","iopub.status.idle":"2022-01-25T08:01:20.524052Z","shell.execute_reply.started":"2022-01-25T08:01:20.517851Z","shell.execute_reply":"2022-01-25T08:01:20.522957Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nnow I need a machinery for cleaning the text\nI borrowed this function from\nhttps://github.com/natsakh/Data-Analysis/blob/main/Pr_4/4.2 - Text cleaning with RegEx.ipynb\nand slightly modified it\nI think that hashtags can be meaningful when it comes to tweeter (unlike in python :) )\nso I commented out the 4th line and removed # from punct symbols\n\"\"\"\nstop_words = stopwords.words(\"english\")\npunctuation_symbols = ''.join(string.punctuation.split('#'))\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(\"@\\S+\", ' ', text)\n    text = re.sub(\"https*\\S+\", ' ', text)\n    #text = re.sub(\"#\\S+\", \" \", text)\n    text = re.sub(\"\\d\", \" \", text)\n    text = re.sub('[%s]' % re.escape(punctuation_symbols), ' ', text)\n    text = re.sub('\\n', ' ', text)\n    text = re.sub('\\s{2,}',' ', text)\n    text = ' '.join([word for word in text.split() if word not in stop_words])\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:27.701168Z","iopub.execute_input":"2022-01-25T08:01:27.701700Z","iopub.status.idle":"2022-01-25T08:01:27.712468Z","shell.execute_reply.started":"2022-01-25T08:01:27.701664Z","shell.execute_reply":"2022-01-25T08:01:27.711323Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#let's check it\ns = 'Noisy Text Example with a lot of numbers and special symbols: 12345, #abc, @@qwerty, http://xyz. Data cleaning Done'\nprint(clean_text(s))","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:31.737261Z","iopub.execute_input":"2022-01-25T08:01:31.737771Z","iopub.status.idle":"2022-01-25T08:01:31.743430Z","shell.execute_reply.started":"2022-01-25T08:01:31.737734Z","shell.execute_reply":"2022-01-25T08:01:31.742423Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"positive = positive.apply(clean_text)\nnegative = negative.apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:34.162575Z","iopub.execute_input":"2022-01-25T08:01:34.163081Z","iopub.status.idle":"2022-01-25T08:01:37.100365Z","shell.execute_reply.started":"2022-01-25T08:01:34.163046Z","shell.execute_reply":"2022-01-25T08:01:37.099727Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#we don't need df's anymore. let's convert them to strings\npos_text = \"\"\nneg_text = \"\"\nfor i in positive:\n    pos_text += i\n    pos_text += ' '\nfor i in negative:\n    neg_text += i\n    neg_text += ' '","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:42.960449Z","iopub.execute_input":"2022-01-25T08:01:42.961159Z","iopub.status.idle":"2022-01-25T08:01:42.993006Z","shell.execute_reply.started":"2022-01-25T08:01:42.961105Z","shell.execute_reply":"2022-01-25T08:01:42.992022Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#now we're ready to tokenize 'n' lemmatize\n#recall that I've already removed stopwords \npos_words = word_tokenize(pos_text)\nneg_words = word_tokenize(neg_text)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:01:48.930822Z","iopub.execute_input":"2022-01-25T08:01:48.931711Z","iopub.status.idle":"2022-01-25T08:01:55.947876Z","shell.execute_reply.started":"2022-01-25T08:01:48.931669Z","shell.execute_reply":"2022-01-25T08:01:55.946872Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"lemmatizer = WordNetLemmatizer()\ndef lem(tokens):\n    return [lemmatizer.lemmatize(t) for t in tokens]","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:02:33.960120Z","iopub.execute_input":"2022-01-25T08:02:33.960420Z","iopub.status.idle":"2022-01-25T08:02:33.965364Z","shell.execute_reply.started":"2022-01-25T08:02:33.960390Z","shell.execute_reply":"2022-01-25T08:02:33.964568Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"lemmed_pos = lem(pos_words)\nlemmed_neg = lem(neg_words)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:03:24.390654Z","iopub.execute_input":"2022-01-25T08:03:24.390945Z","iopub.status.idle":"2022-01-25T08:03:30.244661Z","shell.execute_reply.started":"2022-01-25T08:03:24.390916Z","shell.execute_reply":"2022-01-25T08:03:30.243601Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"#let's visualize\n\ndef visualize(l):\n    text = ''\n    for w in l:\n        text += w + ' '\n    wordcloud = WordCloud(width=600, height=400, background_color=\"white\").generate(text)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:05:32.069389Z","iopub.execute_input":"2022-01-25T08:05:32.069686Z","iopub.status.idle":"2022-01-25T08:05:32.082066Z","shell.execute_reply.started":"2022-01-25T08:05:32.069657Z","shell.execute_reply":"2022-01-25T08:05:32.080979Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"visualize(lemmed_pos)\nvisualize(lemmed_neg)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:06:11.648691Z","iopub.execute_input":"2022-01-25T08:06:11.649032Z","iopub.status.idle":"2022-01-25T08:06:20.127468Z","shell.execute_reply.started":"2022-01-25T08:06:11.648995Z","shell.execute_reply":"2022-01-25T08:06:20.126576Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#now bigrams\ndef visualize_bi(l):\n    text = ''\n    for w in l:\n        text += w + ' '\n    wordcloud = WordCloud(width=600, height=400, background_color=\"white\",collocation_threshold = 3).generate(text)\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:07:59.179830Z","iopub.execute_input":"2022-01-25T08:07:59.180527Z","iopub.status.idle":"2022-01-25T08:07:59.185679Z","shell.execute_reply.started":"2022-01-25T08:07:59.180468Z","shell.execute_reply":"2022-01-25T08:07:59.185073Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"visualize_bi(lemmed_pos)\nvisualize_bi(lemmed_neg)","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:08:18.123619Z","iopub.execute_input":"2022-01-25T08:08:18.124312Z","iopub.status.idle":"2022-01-25T08:08:27.263005Z","shell.execute_reply.started":"2022-01-25T08:08:18.124268Z","shell.execute_reply":"2022-01-25T08:08:27.261872Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#fertig","metadata":{"execution":{"iopub.status.busy":"2022-01-25T08:09:00.900337Z","iopub.execute_input":"2022-01-25T08:09:00.900667Z","iopub.status.idle":"2022-01-25T08:09:00.904722Z","shell.execute_reply.started":"2022-01-25T08:09:00.900633Z","shell.execute_reply":"2022-01-25T08:09:00.903700Z"},"trusted":true},"execution_count":20,"outputs":[]}]}