{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## OK... LET CHOICE 2 DIFFERENT LABELS FROM DATA","metadata":{"id":"UvldSt9RuT_a"}},{"cell_type":"code","source":"import nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n  \nurl='../input/classification-of-class-on-basis-of-text/root2ai - Data.csv'\ndf = pd.read_csv(url)\n\nU=list(df['Target'])\nA = list(set(df['Target']))\n\n#Let's chose 2 very different labels, for example Bigdata and Cyber Security\n\nV = list(df['Text'])\n\nR = [[V[i],U[i]] for i in range(len(U))]\nnew_frame = [i for i in R if i[1]== 'Bigdata' or i[1]== 'Cyber Security']\nnew_frame_texts = [i[0] for i in new_frame]\nnew_frame_labels = [i[1] for i in new_frame]\nnew_frame_labels_asreal = []\nfor i in new_frame_labels:\n    if i == 'Bigdata':\n      new_frame_labels_asreal.append(1.0)\n    elif i == 'Cyber Security':\n      new_frame_labels_asreal.append(0.0)\n# Okey, now we have new dataframe with labels Bigdata = 1.0 and Cyber Security = 0.0\ndf = pd.DataFrame(data = {'Text':new_frame_texts , 'Target':new_frame_labels_asreal})\n\n#Let's watch how distributed labels\nnumber_of_bd = 0\nnumber_of_cs = 0\nfor i in list(df['Target']):\n  if float(i) == 1.0:\n    number_of_bd+=1\n  elif float(i) == 0.0:\n    number_of_cs +=1\n\ntext1 = ''\nfor i in new_frame_texts:\n  text1+=i\n\ndf\n","metadata":{"id":"Ip83J3TCtaQk","outputId":"8c8812f6-7611-4c81-cbb3-afa227fba7bd","execution":{"iopub.status.busy":"2022-02-05T17:19:54.951654Z","iopub.execute_input":"2022-02-05T17:19:54.951982Z","iopub.status.idle":"2022-02-05T17:19:55.043771Z","shell.execute_reply.started":"2022-02-05T17:19:54.951948Z","shell.execute_reply":"2022-02-05T17:19:55.042952Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# SOME PREPROCESSING WITH **NLTK**","metadata":{"id":"0mKCDtkuuYfb"}},{"cell_type":"code","source":"from nltk import WordNetLemmatizer\nimport re\nimport string\n\nU = list(df['Text'])\nlemmatizer = nltk.WordNetLemmatizer()\nV = list(df['Target'])\n\nU_f = []\nfor j in U:\n  R = nltk.word_tokenize(j)\n  Y = [lemmatizer.lemmatize(i) for i in R]\n  StrY = \" \".join(Y)\n  U_f.append(StrY)\n\nstop_words = stopwords.words(\"english\")\n\ndef cleaning(data):\n    \n    filtered = re.sub(\"@\\S+\", \" \", data)  # remove mentions\n    filtered = re.sub(\"https*\\S+\", \" \", filtered) # remove url\n    filtered = re.sub(\"#\\S+\", \" \", filtered) # remove hashtags\n    filtered = re.sub(\"\\d\", \" \", filtered) # remove all numbers\n    filtered = re.sub('[%s]+' % re.escape(string.punctuation), ' ', filtered)  # remove punctuation\n    filtered = re.sub('\\n', ' ', filtered) # remove new lines       \n    filtered = re.sub('\\s{2,}',' ', filtered) # remove extra spaces\n    \n    filtered = filtered.lower()\n    \n    filtered = ' '.join([word for word in filtered.split(' ') if word not in stop_words and len(word)>1 and word.isalpha() == True])\n    \n    return filtered\n\ndf = pd.DataFrame(data = {'Text':[cleaning(i) for i in U_f], 'Target':V})\n\ndf\n","metadata":{"id":"B77VHWfZuYFO","outputId":"75c44257-8a8a-46d8-e589-39c402d02906","execution":{"iopub.status.busy":"2022-02-05T17:19:59.758562Z","iopub.execute_input":"2022-02-05T17:19:59.759087Z","iopub.status.idle":"2022-02-05T17:20:01.097692Z","shell.execute_reply.started":"2022-02-05T17:19:59.759053Z","shell.execute_reply":"2022-02-05T17:20:01.097056Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Scikit-learn methods for creating train and test dataset's","metadata":{"id":"X5wLoigyulWf"}},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nX_train, X_test, y_train, y_test = train_test_split(df['Text'], \n                                                    df['Target'], test_size=0.1 ,\n                                                    random_state=42)\n\n\nX_train, X_test, y_train, y_test = X_train, X_test, np.asarray(y_train).astype('float32'), np.asarray(y_test).astype('float32')\n\n\n\nvectorizer=TfidfVectorizer()\nvectorizer.fit(df['Text'])\nX_train_vectorized = vectorizer.transform(X_train)\nX_train_vectorized = X_train_vectorized.toarray()\n\n\nX_train_vectorized.shape\n","metadata":{"id":"0n4vq-9runDu","outputId":"8cca309e-1f66-4fdc-e0d0-67e302978f8b","execution":{"iopub.status.busy":"2022-02-05T17:20:05.916555Z","iopub.execute_input":"2022-02-05T17:20:05.917094Z","iopub.status.idle":"2022-02-05T17:20:06.223879Z","shell.execute_reply.started":"2022-02-05T17:20:05.917059Z","shell.execute_reply":"2022-02-05T17:20:06.222798Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## FINALLY... REWORKED MODEL FOR THIRD TASK","metadata":{"id":"fQ-xtGcnury5"}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\n\n\nX_train_vectorized = keras.preprocessing.sequence.pad_sequences(X_train_vectorized, maxlen =80)\n\nmodel = keras.Sequential()\nmodel.add(layers.Embedding(10000, 64)) \nmodel.add(layers.Bidirectional(layers.LSTM(32, return_sequences=True)))\nmodel.add(layers.Bidirectional(layers.LSTM(32, return_sequences=False)))\nmodel.add(layers.Dense(1, activation = 'sigmoid'))\n\nmodel.compile(\n    optimizer='adam',\n              loss=\"binary_crossentropy\",\n              metrics=[\"accuracy\"],\n)\nmodel.summary()\n\n#my_callbacks = [tf.keras.callbacks.EarlyStopping(patience=5)]\n\n\nhistory = model.fit(X_train_vectorized, y_train, batch_size=16, epochs=50, validation_split=0.2, callbacks = [tf.keras.callbacks.EarlyStopping(patience=5)])\n\n","metadata":{"id":"C14bKh2uuriT","outputId":"4899353b-3469-4de1-9d4c-14ae2606622f","execution":{"iopub.status.busy":"2022-02-05T17:22:21.977766Z","iopub.execute_input":"2022-02-05T17:22:21.978786Z","iopub.status.idle":"2022-02-05T17:33:19.139412Z","shell.execute_reply.started":"2022-02-05T17:22:21.978726Z","shell.execute_reply":"2022-02-05T17:33:19.138448Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nX_test_vectorized = vectorizer.transform(X_test.apply(lambda x: np.str_(x)))\nX_test_vectorized = X_test_vectorized.toarray()\nscore = model.evaluate(X_test_vectorized, y_test, verbose=0) \n\nprint('Test score:', score[0]) \nprint('Test accuracy:', score[1])","metadata":{"execution":{"iopub.status.busy":"2022-02-05T17:34:21.117569Z","iopub.execute_input":"2022-02-05T17:34:21.118010Z","iopub.status.idle":"2022-02-05T17:35:19.242591Z","shell.execute_reply.started":"2022-02-05T17:34:21.117977Z","shell.execute_reply":"2022-02-05T17:35:19.241732Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"history_dict = history.history\nhistory_dict.keys()\nhistory_dict = history.history\nloss_values = history_dict[\"loss\"]\nval_loss_values = history_dict[\"val_loss\"]\nepochs = range(1, len(loss_values) + 1)\nplt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")\nplt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-05T17:35:43.946185Z","iopub.execute_input":"2022-02-05T17:35:43.946494Z","iopub.status.idle":"2022-02-05T17:35:44.193890Z","shell.execute_reply.started":"2022-02-05T17:35:43.946461Z","shell.execute_reply":"2022-02-05T17:35:44.192937Z"},"trusted":true},"execution_count":26,"outputs":[]}]}