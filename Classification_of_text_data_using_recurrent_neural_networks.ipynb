{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing all necessary libraries and our data"
      ],
      "metadata": {
        "id": "uT_dQOE10cQi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHc3F-2ugRAm",
        "outputId": "fec21b52-c4b4-4610-e744-b09c4797a3d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "WUFR76Xsgy22"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GxhHlAcwgzs_"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/TheSocialDilemma/TheSocialDilemma.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spliting our data into 2 equal parts"
      ],
      "metadata": {
        "id": "zNuw4wTK0kxd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ni83q9kfg44B"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['Sentiment'], test_size=0.50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removing neutral comments"
      ],
      "metadata": {
        "id": "oLwO0_cY0qf3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "1cZqqRCGg7p5"
      },
      "outputs": [],
      "source": [
        "X_train = X_train[(df.Sentiment == 'Positive') | (df.Sentiment == 'Negative')]\n",
        "X_test = X_test[(df.Sentiment == 'Positive') | (df.Sentiment == 'Negative')]\n",
        "y_train = y_train[(df.Sentiment == 'Positive') | (df.Sentiment == 'Negative')]\n",
        "y_test = y_test[(df.Sentiment == 'Positive') | (df.Sentiment == 'Negative')]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cleaninig text"
      ],
      "metadata": {
        "id": "NT0cwTLl0tBi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "8G9PP3_rg-jR"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words(\"english\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NCCmlqFuhBII"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"@\\S+\", \" \", text)\n",
        "    text = re.sub(\"https*\\S+\", \" \", text)\n",
        "    text = re.sub(\"#\\S+\", \" \", text)\n",
        "    text = re.sub(\"\\d\", \" \", text)\n",
        "    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n",
        "    text = re.sub('\\n', ' ', text)\n",
        "    text = re.sub('\\s{2,}',' ', text)\n",
        "    text = ' '.join([word for word in text.split(' ') if word not in stop_words])\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0ulnR_a_hDaR"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.apply(lambda x:clean_text(x))\n",
        "X_test = X_test.apply(lambda x:clean_text(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting \"Positive\" and \"Negative\" into 1.0 and 0.0 for further calculations"
      ],
      "metadata": {
        "id": "jQIcQhCP0wUq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "tpt1dyGDhJCr"
      },
      "outputs": [],
      "source": [
        "def MakingLabel(text):\n",
        "  if text == \"Positive\":\n",
        "    return 1.0\n",
        "  if text == \"Negative\":\n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Bkx60LvZhK3e"
      },
      "outputs": [],
      "source": [
        "y_train = y_train.apply(lambda x:MakingLabel(x))\n",
        "y_test = y_test.apply(lambda x:MakingLabel(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making a dictionary of the whole text to create sequences"
      ],
      "metadata": {
        "id": "lLq_q4aB00d6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "uJfbocJRhNZJ"
      },
      "outputs": [],
      "source": [
        "text = ''\n",
        "for i in X_train:\n",
        "    text += i + ' '\n",
        "for i in X_test:\n",
        "    text += i + ' '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "pCI3YMNVhQ0E"
      },
      "outputs": [],
      "source": [
        "def tokenization(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "JchfNFMjhSs0"
      },
      "outputs": [],
      "source": [
        "def lemmatizer(text):\n",
        "    lemm_text = [WordNetLemmatizer().lemmatize(word) for word in text]\n",
        "    return lemm_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "5e3tJnjZhVZC"
      },
      "outputs": [],
      "source": [
        "dict_full = list(set(lemmatizer(tokenization(text))))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making sequences"
      ],
      "metadata": {
        "id": "uI-XB2Ue03Wf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "fVb5gL6EitQ2"
      },
      "outputs": [],
      "source": [
        "def sequences(text):\n",
        "    sequence = [dict_full.index(i) for i in lemmatizer(tokenization(text))]\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "uQVwTWUmhW5w"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.apply(lambda x:sequences(x))\n",
        "X_test = X_test.apply(lambda x:sequences(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorizing sequences"
      ],
      "metadata": {
        "id": "cwP068wn06Xh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ItX5OzIshYKp"
      },
      "outputs": [],
      "source": [
        "def vectorize_sequences(sequences, dimension=11000):\n",
        "    results = np.zeros((len(sequences), dimension))\n",
        "    for i, sequence in enumerate(sequences):\n",
        "        for j in sequence:\n",
        "            results[i, j] = 1.\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "VOjBkvo9haE5"
      },
      "outputs": [],
      "source": [
        "X_train = vectorize_sequences(X_train)\n",
        "X_test = vectorize_sequences(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_Mg1zEyPhbZo"
      },
      "outputs": [],
      "source": [
        "y_train = np.asarray(y_train).astype(\"float32\")\n",
        "y_test = np.asarray(y_test).astype(\"float32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making our model"
      ],
      "metadata": {
        "id": "V7EYhUdD08m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = 200\n",
        "\n",
        "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "0isNZBNKvMFt"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "zr9KykcRhcyf"
      },
      "outputs": [],
      "source": [
        "emb_dim = 128\n",
        "max_features = 11000\n",
        "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(layers.Embedding(max_features, emb_dim))\n",
        "model.add(layers.LSTM(128))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1YMlcVai9Sq",
        "outputId": "de88922a-9dcd-46c3-9d88-8898ca2d51b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "41/41 [==============================] - 51s 1s/step - loss: 0.6017 - accuracy: 0.7274 - val_loss: 0.5762 - val_accuracy: 0.7399\n",
            "Epoch 2/5\n",
            "41/41 [==============================] - 42s 911ms/step - loss: 0.5870 - accuracy: 0.7274 - val_loss: 0.5742 - val_accuracy: 0.7399\n",
            "Epoch 3/5\n",
            "41/41 [==============================] - 37s 899ms/step - loss: 0.5864 - accuracy: 0.7274 - val_loss: 0.5732 - val_accuracy: 0.7399\n",
            "Epoch 4/5\n",
            "41/41 [==============================] - 38s 895ms/step - loss: 0.5867 - accuracy: 0.7274 - val_loss: 0.5734 - val_accuracy: 0.7399\n",
            "Epoch 5/5\n",
            "41/41 [==============================] - 36s 872ms/step - loss: 0.5868 - accuracy: 0.7274 - val_loss: 0.5750 - val_accuracy: 0.7399\n"
          ]
        }
      ],
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train,\n",
        "                    y_train,\n",
        "                    epochs=5,\n",
        "                    batch_size=128,\n",
        "                    validation_split=0.2,\n",
        "                    callbacks=[callback],\n",
        "                    shuffle = True,\n",
        "                    verbose = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "56zjJUOhjAMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9660ff12-4343-4127-b6f9-1a25799269b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 128)         1408000   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 128)               131584    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,539,713\n",
            "Trainable params: 1,539,713\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "6xRbpHUKonBO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "218ac1e8-7317-4cd9-a5e1-bdd2885cb5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test score: 0.5937301516532898\n",
            "Test accuracy: 0.7240274548530579\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(X_test, y_test, verbose=0) \n",
        "print('Test score:', score[0]) \n",
        "print('Test accuracy:', score[1])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Classification of text data using recurrent neural networks.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}